import os
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ğŸ—‚ï¸ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
processed_folder = "data/processed"
faiss_folder = "vectorstores/arabic_faiss"

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Multilingual)
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# ğŸ“„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n", ".", "ØŒ", " "]
)

# ğŸ“š ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
all_documents = []

for filename in os.listdir(processed_folder):
    if filename.endswith(".txt"):
        path = os.path.join(processed_folder, filename)
        loader = TextLoader(path, encoding="utf-8")
        docs = loader.load()
        split_docs = text_splitter.split_documents(docs)
        all_documents.extend(split_docs)

# ğŸ§  Ø¨Ù†Ø§Ø¡ FAISS vectorstore
print(f"ğŸ”¢ Ø¨Ù†Ø§Ø¡ FAISS vectorstore Ù„Ø¹Ø¯Ø¯: {len(all_documents)} Ù…Ù‚Ø·Ø¹...")
db = FAISS.from_documents(all_documents, embedding_model)

# ğŸ’¾ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
db.save_local(faiss_folder)
print("âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© FAISS ÙÙŠ:", faiss_folder)
