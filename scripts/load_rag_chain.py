import os
from dotenv import load_dotenv
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


def get_rag_chain():
    # ğŸŸ¢ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
    load_dotenv()
    
    # ğŸŸ¢ dummy embedding ÙÙ‚Ø· Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† FAISS
    embedding_model = HuggingFaceEmbeddings(
        model_name="intfloat/e5-small-v2"  # Ù†Ù…ÙˆØ°Ø¬ Ø®ÙÙŠÙ ÙˆÙ…Ø¯Ø¹ÙˆÙ…
    )

    # ğŸŸ¢ ØªØ­Ù…ÙŠÙ„ vectorstore Ø§Ù„Ø¬Ø§Ù‡Ø² Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯
    vectorstore = FAISS.load_local(
        "vectorstores/arabic_faiss",
        embedding_model,
        allow_dangerous_deserialization=True
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # ğŸ§  Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
    prompt_template = PromptTemplate(
        template="""
You are a smart, helpful assistant. Your goal is to answer the userâ€™s question as clearly and accurately as possible 
by using the retrieved context provided below. If the answer is not directly in the context, you should still 
use reasoning, inference, and general knowledge to produce a helpful and logical answer.

Never mention that the context is missing or insufficient. Never say "I don't know." Always try your best to answer 
the question meaningfully and informatively.

---------------------
Context:
{context}
---------------------

Question: {question}

Answer:""",
        input_variables=["context", "question"]
    )

    # ğŸ§  Ø¥Ø¹Ø¯Ø§Ø¯ LLM Ù…Ù† OpenRouter (gpt-4o-mini)
    llm = ChatOpenAI(
        model="openai/gpt-4o-mini",
        temperature=0.2,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()],
        openai_api_key=os.getenv("OPENROUTER_API_KEY"),
        openai_api_base="https://openrouter.ai/api/v1"
    )

    # ğŸ” RAG chain
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt_template}
    )

    return rag_chain
