import os

# ğŸ—‚ï¸ ØªØ¹Ø±ÙŠÙ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
folders = [
    "data/raw",
    "data/processed",
    "scripts",
    "models/embedding_model",
    "vectorstores/arabic_faiss",
    "utils"
]

# ğŸ“„ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
files = {
    "requirements.txt": """langchain
openai
streamlit
python-docx
faiss-cpu
tiktoken
transformers
sentence-transformers
python-dotenv
""",
    ".env": "# API keys Ù‡Ù†Ø§ Ù„Ùˆ Ù‡ØªØ³ØªØ®Ø¯Ù… OpenAI Ø£Ùˆ OpenRouter",
    "README.md": "# Arabic RAG System\n\nÙ…Ø´Ø±ÙˆØ¹ Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ù†ØµÙŠØ© Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LangChain ÙˆFAISS.",
    "utils/file_loader.py": "# â¬…ï¸ Ø¯ÙˆØ§Ù„ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª txt Ùˆ docx",
    "utils/cleaner.py": "# â¬…ï¸ Ø¯ÙˆØ§Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ",
    "utils/splitter.py": "# â¬…ï¸ Ø¯ÙˆØ§Ù„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ chunks",
    "scripts/1_extract_data.py": "# â¬…ï¸ Ø³ÙƒØ±Ø¨Øª Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª raw",
    "scripts/2_clean_and_split.py": "# â¬…ï¸ Ø³ÙƒØ±Ø¨Øª Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§",
    "scripts/3_build_vectorstore.py": "# â¬…ï¸ Ø³ÙƒØ±Ø¨Øª Ù„Ø¥Ù†Ø´Ø§Ø¡ FAISS vectorstore",
    "scripts/4_load_rag_chain.py": "# â¬…ï¸ Ø³ÙƒØ±Ø¨Øª Ù„ØªØ­Ù…ÙŠÙ„ RAG chain",
    "scripts/5_run_interface.py": "# â¬…ï¸ ÙˆØ§Ø¬Ù‡Ø© Streamlit"
}

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
for folder in folders:
    os.makedirs(folder, exist_ok=True)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª
for file_path, content in files.items():
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(content)

print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­!")
